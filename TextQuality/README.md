# TextQuality


[![Python](https://img.shields.io/badge/Python-Used-blue.svg)](https://shields.io/#[![/)

## Abstract
As a response to an emerging interest in automatic analysis of L2 text quality (i.e., semantic-pragmatic aspects of language use), the present study compares various NLP techniques for text similarity measurement, employing written production by learners of Korean, a lesser-studied and computationally challenging language in this respect. We choose NLP techniques representative of topic modelling (LSA and LDA) and word embedding (Word2Vec and BERT) and apply them to learner writing (with two essay topics) to see how informative the similarity scores of learner writing are of proficiency. We note three major findings: (i) asymmetric degrees to which the similarity scores explain the proficiency scores—Word2Vec demonstrating the best performance in both topics, LSA and BERT achieving partial success only in one topic, and LDA being the least effective; (ii) sensitivity of model performance to essay topics (particularly to the repetitive use of words relevant to the topics); (iii) global limitation to capturing individual variations. These results suggest that, when evaluating L2 text quality with NLP techniques, researchers should pay attention to how each technique’s algorithm operate in conjunction with learner language characteristics.

### Skills
-------
Machine Learning & NLP & Statistics

- Computer Languages: Python
- DataBase: csv, txt
- Machine Learning: LDA, LSA, Word2Vec, and BERT
- NLP-based methods: Morpheme analysis, Pre-Processing (tokenization, lemetazation, N-gram, window size), etc.
- Statistics: Proficiency scores
- Tools: PyCharm, Jupyter Notebook

